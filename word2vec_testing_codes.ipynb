{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1EUeS10AooR",
        "outputId": "5fe4536e-30c1-410c-f767-3c44100826de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsgpqHGFFo-E",
        "outputId": "4b358f62-8d4f-41eb-f455-8026eebee22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sumy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdIu-QDgxuyY",
        "outputId": "5bd5adda-20ab-4766-f9c0-c52c74e80ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.27.1)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n",
            "Building wheels for collected packages: breadability, docopt, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21696 sha256=f5b920378164aa578dd2d9a370e83953b3e0059f4d198c51c2d1a3ae96070adf\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=08139f6516787f24f85c56d5db931acf558462643f277de9befd6b589826e829\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=5b083e367cebe71f864eb0e9a42e79011db9b0e2221de75f29c938f846f14691\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/57/cc/290c5252ec97a6d78d36479a3c5e5ecc76318afcb241ad9dbe\n",
            "Successfully built breadability docopt pycountry\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyiC561PCNPI",
        "outputId": "ce1c9cf7-b7d5-42e6-bd65-690c1fe3e975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n"
      ],
      "metadata": {
        "id": "-N8idGpGWKS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape the content of a web page (content is the eneregy certificate and not the report)\n",
        "def scrape_webpage(url):\n",
        "    response = requests.get(url)\n",
        "    content = response.text\n",
        "    return content\n",
        "\n",
        "# Function to summarize the content using LexRank algorithm\n",
        "def summarize_content(content, sentences_count=3):\n",
        "    parser = HtmlParser.from_string(content, None, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary = summarizer(parser.document, sentences_count)\n",
        "    return \" \".join(str(sentence) for sentence in summary)\n",
        "\n",
        "# URL of the web page to scrape and summarize\n",
        "url = \"https://find-energy-certificate.service.gov.uk/energy-certificate/9671-3064-0910-0700-9495\"  # Replace with the actual URL of the web page you want to scrape\n",
        "\n",
        "# Scrape the web page content\n",
        "content = scrape_webpage(url)\n",
        "\n",
        "# Summarize the content\n",
        "summary = summarize_content(content)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYKLqJElyZzl",
        "outputId": "e6b45b98-676a-4b5e-e5cb-bf658cc60dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Properties can be let if they have an energy rating from A+ to E. This property’s current energy rating is E. the efficiency of the property’s heating system power station efficiency for electricity the energy used to produce the fuel and deliver it to the property\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a function for cleaning and preprocessing the text data\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Convert to lowercase\n",
        "    summary = text.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    summary = re.sub(r'[^a-zA-Z0-9\\s]', '', summary)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(summary)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Preprocess the text\n",
        "cleaned_text = preprocess_text(summary)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = gensim.models.Word2Vec([cleaned_text], min_count=1, vector_size=100)\n",
        "\n",
        "# Vectorize the words\n",
        "word_vectors = []\n",
        "existing_words = set()\n",
        "\n",
        "for sentence in cleaned_text:\n",
        "    for word in sentence:\n",
        "        if word in model.wv.key_to_index and word not in existing_words:\n",
        "            word_vectors.append(model.wv.get_vector(word))\n",
        "\n",
        "# Create a dataframe to store the word vectors\n",
        "word_vectors_array = np.array(word_vectors)\n",
        "reshaped_array = np.reshape(word_vectors_array, (1, -1))\n",
        "df = pd.DataFrame(reshaped_array)\n",
        "#df = pd.DataFrame(data=word_vectors, index=existing_words)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt5W9YILz9zN",
        "outputId": "e4b21c7b-c91e-4343-b047-187e657976f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0         1         2         3         4         5         6     \\\n",
            "0 -0.008246  0.009299 -0.000196 -0.001968  0.004607 -0.004093  0.002744   \n",
            "\n",
            "       7        8         9     ...      2790      2791      2792      2793  \\\n",
            "0  0.006943  0.00607 -0.007512  ... -0.007427 -0.001065 -0.000793 -0.002567   \n",
            "\n",
            "       2794      2795      2796      2797      2798      2799  \n",
            "0  0.009686 -0.000458  0.005875 -0.007446 -0.002506 -0.005546  \n",
            "\n",
            "[1 rows x 2800 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://find-energy-certificate.service.gov.uk/energy-certificate/9243-1656-8157-5614-7032\"\n",
        "\n",
        "response = requests.get(url)\n",
        "content = response.text\n",
        "\n",
        "soup = BeautifulSoup(content, 'html.parser')\n",
        "text = soup.get_text()\n",
        "\n",
        "# Define a function for cleaning and preprocessing the text data\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    cleaned_text = [word for word in words if not word.isdigit()]\n",
        "\n",
        "    # Join the cleaned words back into a single string\n",
        "    cleaned_text = \" \".join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'text_data' contains the extracted text from the web source\n",
        "cleaned_text = preprocess_text(text)\n",
        "\n",
        "print(cleaned_text)\n",
        "\n",
        "#model = gensim.models.Word2Vec(cleaned_text, min_count=1, vector_size=100)\n",
        "\n",
        "\n",
        "\n",
        "# Assuming 'text_data' contains the extracted text from the web source\n",
        "#cleaned_sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "#vecs = pd.DataFrame([model[word] for sentence in cleaned_sentences for word in sentence if word in model.vocab])\n",
        "\n",
        "#word_vectors = vecs.values\n",
        "#words = [word for sentence in cleaned_sentences for word in sentence if word in model.vocab]\n",
        "#df = pd.DataFrame(cleaned_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb4njnxpWZZ9",
        "outputId": "84e59a2b-04c8-4c65-8518-dfd76e6c2296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "energy performance certificate epc find energy certificate govuk cooky find energy certificate use essential cooky make service work wed also like use analytics cooky understand use service make improvement accept analytics cooky reject analytics cooky view cooky youve accepted analytics cooky change cookie setting time youve rejected analytics cooky change cookie setting time hide cookie message skip main content govuk find energy certificate energy performance certificate epc recommendation report report content energy rating epc recommendation property report detail assessor detail report property share report email copy link clipboard print su5355 vicar lane leeds ls1 6ba report number 92431656815756147032 valid 3 february 2032 energy rating epc property current energy rating information property energy performance see epc property recommendation make change improve property energy efficiency recommended improvement grouped estimated time would take change pay assessor may also make additional recommendation recommendation marked low medium high show potential impact change reducing property carbon emission change pay within 3 year recommendation potential impact improve insulation hws storage low add time control heating system medium space solar gain limit defined ncm exceeded might cause overheating consider solar control measure application reflective coating shading device window medium add optimum startstop heating system medium change pay within 3 7 year recommendation potential impact window high uvalues consider installing secondary glazing medium add local temperature control heating system medium add weather compensation control heating system medium loft space poorly insulated installimprove insulation medium add local time control heating system medium solid wall poorly insulated introduce improve internal wall insulation medium carry pressure test identify treat identified air leakage enter result epc calculation medium change pay 7 year recommendation potential impact glazing poorly insulated replaceimprove glazing andor frame medium consider installing air source heat pump high consider installing ground source heat pump high consider installing building mounted wind turbine low consider installing solar water heating low additional recommendation recommendation potential impact consider replacing t8 lamp led low property report detail report issued 4 february 2022 total useful floor area 864 square metre building environment heating natural ventilation calculation tool designbuilder software ltd designbuilder sbem v618 sbem v56b0 assessor detail assessor name ashton kaziboni telephone 01327 811166 email ashtonkazibonisocoteccom employer name socotec ltd employer address henge barn pury hill business park alderton road assessor id stro034300 assessor declaration assessor related owner property accreditation scheme stroma certification ltd report property aware previous report property listed please contact u dluhcdigitalserviceslevellingupgovuk call helpdesk 020 3829 0748 monday friday 9am 5pm related report property support link accessibility statement cooky service give feedback service performance content available open government licence v30 except otherwise stated crown copyright\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://find-energy-certificate.service.gov.uk/energy-certificate/9243-1656-8157-5614-7032\"\n",
        "\n",
        "response = requests.get(url)\n",
        "content = response.text\n",
        "\n",
        "soup = BeautifulSoup(content, 'html.parser')\n",
        "text = soup.get_text()\n",
        "\n",
        "# Define a function for cleaning and preprocessing the text data\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Preprocess the text\n",
        "cleaned_text = preprocess_text(text)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = gensim.models.Word2Vec([cleaned_text], min_count=1, vector_size=100)\n",
        "\n",
        "# Vectorize the words\n",
        "word_vectors = []\n",
        "existing_words = set()\n",
        "\n",
        "for sentence in cleaned_text:\n",
        "    for word in sentence:\n",
        "        if word in model.wv.key_to_index and word not in existing_words:\n",
        "            word_vectors.append(model.wv.get_vector(word))\n",
        "\n",
        "# Create a dataframe to store the word vectors\n",
        "word_vectors_array = np.array(word_vectors)\n",
        "reshaped_array = np.reshape(word_vectors_array, (1, -1))\n",
        "df = pd.DataFrame(reshaped_array)\n",
        "#df = pd.DataFrame(data=word_vectors, index=existing_words)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g83Y3yb4osBQ",
        "outputId": "e21883d1-ee36-4b64-c752-dc1ab2c64894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0         1         2         3         4         5         6     \\\n",
            "0  0.007838  0.009745  0.000386  0.001561  0.003197 -0.003717  0.007622   \n",
            "\n",
            "       7         8         9     ...      9290    9291      9292      9293  \\\n",
            "0  0.004565  0.000164  0.005176  ... -0.005351 -0.0094  0.006982 -0.005469   \n",
            "\n",
            "       9294      9295      9296     9297     9298      9299  \n",
            "0 -0.009151  0.002698 -0.004932 -0.00741  0.00972 -0.001602  \n",
            "\n",
            "[1 rows x 9300 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory =\n",
        "file_list = os.listdir(directory)\n",
        "response = requests.get(url)\n",
        "content = response.text\n",
        "\n",
        "soup = BeautifulSoup(content, 'html.parser')\n",
        "text = soup.get_text()\n",
        "\n",
        "# Define a function for cleaning and preprocessing the text data\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Preprocess the text\n",
        "cleaned_text = preprocess_text(text)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = gensim.models.Word2Vec([cleaned_text], min_count=1, vector_size=100)\n",
        "\n",
        "# Vectorize the words\n",
        "word_vectors = []\n",
        "existing_words = set()\n",
        "\n",
        "for sentence in cleaned_text:\n",
        "    for word in sentence:\n",
        "        if word in model.wv.key_to_index and word not in existing_words:\n",
        "            word_vectors.append(model.wv.get_vector(word))\n",
        "\n",
        "# Create a dataframe to store the word vectors\n",
        "word_vectors_array = np.array(word_vectors)\n",
        "reshaped_array = np.reshape(word_vectors_array, (1, -1))\n",
        "df = pd.DataFrame(reshaped_array)\n",
        "#df = pd.DataFrame(data=word_vectors, index=existing_words)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "VnrqjnBhs6JN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}